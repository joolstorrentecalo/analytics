{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7eadbd-7adf-4c53-ac07-3c2dec095ff1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# this is a parameter that will get overwritten when run by papermill on a schedules\n",
    "is_local_development = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c91380-795a-4b87-aa26-a02aae5be6c4",
   "metadata": {},
   "source": [
    "# Jupyter JSON upload notebook\n",
    "\n",
    "The goal is to have a template on how to upload data to Snowflake using pandas dataframe upload and a JSON approach.\n",
    "\n",
    "The model leverages or is inspired by the following libraries:\n",
    "\n",
    "- [Gitlab Orchestation Utils](https://gitlab.com/gitlab-data/gitlab-data-utils/-/blob/master/gitlabdata/orchestration_utils.py#L282)\n",
    "- [Data Science Scoring repo](https://gitlab.com/gitlab-data/data-science-projects/propensity-to-contract-and-churn/-/blob/main/prod/scoring_code.ipynb)\n",
    "\n",
    "## Status:\n",
    "\n",
    "20230215 NF: \n",
    "\n",
    "\n",
    "The JSON upload process works but only for smaller files (no more than 16MB). That seriously hampers its usefulness. \n",
    "\n",
    "The data frame upload process took around 19 minutes to execute the upload for a file of 200k rows. It is slow but it works.\n",
    "\n",
    "A parameter and a code to handle production vs local run was added to the template. Also the gsheet write function works. Just wondering if we could pass the value as a parameter instead as of a OS variable. \n",
    "\n",
    "The JSON file had this error regarding size: \n",
    "\n",
    "```\n",
    "ProgrammingError: (snowflake.connector.errors.ProgrammingError) 100069 (22P02): 01aa607c-0405-b753-0000-289d4df5c93a: Error parsing JSON: document is too large, max size 16777216 bytes\n",
    "  File 'stages/b712926c-eb9b-4af5-b02c-a2ae80ce832a/upa_summary.json.gz', line 1, character 16777216\n",
    "  Row 0, column $1\n",
    "  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client.\n",
    "[SQL: copy into raw.sales_analytics.upa_summary_json (jsontext)\n",
    "                         from @raw.sales_analytics.sales_analytics_load\n",
    "                         file_format=(type='json'),\n",
    "                         on_error='abort_statement';]\n",
    "(Background on this error at: https://sqlalche.me/e/14/f405)\n",
    "\n",
    "-----------------------------\n",
    "20230207 NF: The Dataframe upload process works, but the JSON process stills fails with an error \n",
    "\n",
    "```\n",
    "(snowflake.connector.errors.ProgrammingError) 000904 (42000): 01aa2b92-0405-a900-0000-289d4d4d634a: SQL compilation error: error line 1 at position 43\n",
    "invalid identifier 'JSONTEXT'\n",
    "[SQL: copy into raw.sales_analytics.upa_summary (jsontext)\n",
    "                         from @raw.sales_analytics.sales_analytics_load\n",
    "                         file_format=(type='json'),\n",
    "                         on_error='skip_file';]\n",
    "```\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4fedc-eda4-4891-a74f-975d06d25733",
   "metadata": {},
   "source": [
    "## Local variable set up:\n",
    "\n",
    "A local variable in the file .dbt/profiles.yml needs to be created\n",
    "\n",
    "```\n",
    "output:\n",
    "   sales_analytics_local:\n",
    "     type: snowflake\n",
    "     threads: 16\n",
    "     account: gitlab\n",
    "     user: janesmith@gitlab.com # <-- This will be your GitLab email\n",
    "     role: sales_analytics # <-- Talk to your manager, usually, it is JSMITH for Jane Smith\n",
    "     database: RAW\n",
    "     warehouse: DEV_XS # <-- [ANALYST_XS, ENGINEER_XS], depends on your role\n",
    "     schema: SALES_ANALYTICS\n",
    "     authenticator: externalbrowser #\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f0d7c-69dc-4896-819e-263b14ff9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "! pip install  pygsheets\n",
    "! pip install \"pyarrow<5.1.0,>=5.0.0;\"\n",
    "! pip install --upgrade google.cloud\n",
    "! pip install --upgrade pandas_gbq\n",
    "! pip install pyprojroot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ea420-4035-4424-be29-cf7cdb9722e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install gitlabdata --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea41d9-b0a0-415d-998a-9e7c0f3b8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygsheets\n",
    "import configparser\n",
    "import sys\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np                   # v 1.19.2\n",
    "import matplotlib.pyplot as plt      # v 3.3.2\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "# calculate the net_arr bucket of open deals\n",
    "import seaborn as sns\n",
    "from math import floor \n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://pypi.org/project/pyprojroot/\n",
    "from pyprojroot import here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca065e8-5fc5-409a-a428-5eb2d9e3ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gitlabdata.orchestration_utils import (\n",
    "    data_science_engine_factory,\n",
    "    query_dataframe,\n",
    "    snowflake_engine_factory,\n",
    "    snowflake_stage_load_copy_remove,\n",
    "    get_env_from_profile,\n",
    "    dataframe_uploader,\n",
    "    write_to_gsheets,\n",
    "    query_executor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccc542-bd5c-4e93-b7a2-418d2152352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "os.getcwd()\n",
    "\n",
    "# NF: Just to deal with my working directory changing\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "#os.chdir(\"/Users/nfiguera/repos/sales-strategy-and-analytics-business-intelligence/jupyter_dev/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a898e93-3838-4401-842e-320b4336557a",
   "metadata": {},
   "source": [
    "## Create Snowflake engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859777a1-99e7-42a1-a2eb-a895ba919c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine factory can be created using a local role from output\n",
    "# depending on this notebook being run locally or remotely, the \n",
    "# engine is creation process is different\n",
    "\n",
    "if is_local_development:\n",
    "    snowflake_engine = data_science_engine_factory(profile_target=\"sales_analytics_local\")\n",
    "else:\n",
    "    snowflake_engine = snowflake_engine_factory(env, \"sales_analytics\")\n",
    "\n",
    "snowflake_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b356fa-3547-4608-a852-52ff741c6e91",
   "metadata": {},
   "source": [
    "## Excute Snowflake query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed438cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeScriptFromFile(filename, engine):\n",
    "    # Open and read the file as a single buffer\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "\n",
    "    print(filename)\n",
    "    print(len(sqlFile))\n",
    "\n",
    "    results = -1\n",
    "\n",
    "    try:\n",
    "        results = query_dataframe(engine, sqlFile)\n",
    "    except:\n",
    "        print(\"Command did not run\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d611210",
   "metadata": {},
   "outputs": [],
   "source": [
    "upa_summary = executeScriptFromFile('dbt_bob_upa.sql', snowflake_engine)\n",
    "\n",
    "# TEST Total FY Net ARR\n",
    "index = (upa_summary['report_fiscal_year'] == 2023)\n",
    "upa_summary[index].fy_booked_net_arr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec218f-85cf-4ec3-92e2-012799c81475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the credentials of the google service account\n",
    "import json, os\n",
    "\n",
    "if is_local_development:\n",
    "    credentials_path = here('credentials/gsheet_service_file.json')\n",
    "\n",
    "    with open(credentials_path) as f:\n",
    "       service_account_credentials = f.read().replace('\\n', '')\n",
    "\n",
    "    # set the credential as a enviroment variable\n",
    "    os.environ[\"GSHEETS_SERVICE_ACCOUNT_CREDENTIALS\"] = service_account_credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to GSheets\n",
    "sheet_id = '1eRo30S0G4-QkGBdpz7jBmRSRsMHkfysfi_xv2ab_28Q'\n",
    "sheet_name = 'new_nf_testing'\n",
    "write_to_gsheets(sheet_id, sheet_name, upa_summary.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d5ac2-0eb2-4f04-a37a-57104933f1f9",
   "metadata": {},
   "source": [
    "## Test to_sql from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950bdf44-f11e-4056-96ea-1100d7ef7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(upa_summary))\n",
    "\n",
    "# this works\n",
    "dataframe_uploader(\n",
    "    dataframe = upa_summary,\n",
    "    engine = snowflake_engine,\n",
    "    table_name = 'upa_summary',\n",
    "    schema = \"SALES_ANALYTICS\",\n",
    "    if_exists = \"replace\",\n",
    "    add_uploaded_at = False\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c189e-8122-4363-b341-e56a88d05b72",
   "metadata": {},
   "source": [
    "## The JSON Upload process\n",
    "\n",
    "JSON process expects a table with two columns:\n",
    "- JSONTEXT with the JSON file as content\n",
    "- UPDATED AT\n",
    "\n",
    "The table is then processed by DBT models and exposed in prod as a flat table.\n",
    "\n",
    "**JSON upload only work for smaller files!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d64ea-bbf1-4d74-8038-1ba74abb1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# OUTPUT SCORES TO JSON\n",
    "output_filename = here(\"data/upa_summary.json\")\n",
    "\n",
    "# test with only 10 rows\n",
    "output_scores = upa_summary.head(1000)\n",
    "output_scores.to_json(output_filename, orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "# this table is later processed using dbt models\n",
    "json_tablename = 'raw.sales_analytics.upa_summary_json'\n",
    "\n",
    "# creation of target table\n",
    "create_json_table_query = 'CREATE OR REPLACE TABLE {} (jsontext string, updated_at date)'.format(json_tablename)\n",
    "\n",
    "# create or replace existing table with the JSON expected format\n",
    "query_executor(snowflake_engine, create_json_table_query)\n",
    "\n",
    "snowflake_stage_load_copy_remove(\n",
    "    output_filename,\n",
    "    f\"raw.sales_analytics.sales_analytics_load\",\n",
    "    json_tablename,\n",
    "    snowflake_engine,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea4f0e1d1faa5bb96f695d7e31ce7a9817f73b9836258768e692d46b900aef52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
